{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26e18b-ae4f-492e-a0ad-aa6fda1dbc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.io\n",
    "import seaborn as sns\n",
    "from timeit import default_timer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "from utilities import *\n",
    "from nn_conv import NNConv, NNConv_old\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..', '..'))\n",
    "from foundation_pdes.pytorch_net.util import record_data, to_cpu, to_np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8579c-f8f2-44d6-80db-b154127e2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelNN3(torch.nn.Module):\n",
    "    def __init__(self, width_node, width_kernel, depth, ker_in, in_width=1, out_width=1):\n",
    "        super(KernelNN3, self).__init__()\n",
    "        self.depth = depth\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(in_width, width_node)\n",
    "\n",
    "        kernel = DenseNet([ker_in, width_kernel // 2, width_kernel, width_node**2], torch.nn.ReLU)\n",
    "        self.conv1 = NNConv_old(width_node, width_node, kernel, aggr='mean')\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(width_node, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.fc1(x)\n",
    "        for k in range(self.depth):\n",
    "            x = self.conv1(x, edge_index, edge_attr)\n",
    "            if k != self.depth - 1:\n",
    "                x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def plot(pred, y, a_ori, resolution, diff_v_limit=0.01, title=\"\"):\n",
    "    vmin = min(pred.min().item(), y.min().item())\n",
    "    vmax = max(pred.max().item(), y.max().item())\n",
    "    fontsize = 15\n",
    "    add_plot = 0\n",
    "    if a_ori is not None:\n",
    "        add_plot += 1\n",
    "    L2 = myloss(pred.view(1,-1), y.view(1, -1)).item()\n",
    "    mae = nn.L1Loss()(pred.view(1,-1), y.view(1, -1)).item()\n",
    "    fig = plt.figure(figsize=(25,4.5))\n",
    "    plt.subplot(1,3+add_plot,1)\n",
    "    sns.heatmap(to_np_array(pred).reshape(resolution,resolution), square=True, xticklabels=False, vmin=vmin, vmax=vmax, yticklabels=False)\n",
    "    plt.title(f\"{title}: pred, L2={L2:.6f}\", fontsize=fontsize)\n",
    "    plt.subplot(1,3+add_plot,2)\n",
    "    sns.heatmap(to_np_array(y).reshape(resolution,resolution), square=True, xticklabels=False, vmin=vmin, vmax=vmax, yticklabels=False)\n",
    "    plt.title(f\"gt, MAE={mae:.6f}\", fontsize=fontsize)\n",
    "    plt.subplot(1,3+add_plot,3)\n",
    "    sns.heatmap(to_np_array(pred - y).reshape(resolution,resolution), square=True, xticklabels=False, yticklabels=False, vmin=-diff_v_limit, vmax=diff_v_limit, cmap=\"PiYG\")\n",
    "    plt.title(\"pred - gt\", fontsize=fontsize)\n",
    "    if add_plot > 0:\n",
    "        plt.subplot(1,3+add_plot,4)\n",
    "        sns.heatmap(to_np_array(a_ori).reshape(resolution,resolution), square=True, xticklabels=False, yticklabels=False)\n",
    "        plt.title(\"f\", fontsize=fontsize)\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f9551-6bfb-4980-bee9-1c0e5670c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Training')\n",
    "\n",
    "parser.add_argument('--dataset_type', default=\"poisson1.0-32\", type=str,\n",
    "                    help='dataset type')\n",
    "parser.add_argument('--epochs', default=1000, type=int,\n",
    "                    help='Epochs')\n",
    "parser.add_argument('--lr', default=0.0001, type=float,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--inspect_interval', default=100, type=int,\n",
    "                    help='inspect interval')\n",
    "parser.add_argument('--id', default=\"0\", type=str,\n",
    "                    help='ID')\n",
    "try:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    is_jupyter = True\n",
    "    args = parser.parse_args([])\n",
    "    args.dataset_type = \"poisson1.0-64\"\n",
    "except:\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011ddf6-e3c9-4f2b-aa6f-c26495943a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = args.dataset_type\n",
    "resolution = eval(dataset_type.split(\"-\")[1])\n",
    "\n",
    "TRAIN_PATH = 'data/piececonst_r241_N1024_smooth1.mat'\n",
    "TEST_PATH = 'data/piececonst_r241_N1024_smooth2.mat'\n",
    "DATA_PATH = f\"../../data/Poisson1.0_{resolution}/files/\"\n",
    "\n",
    "ms = [200]\n",
    "case = 0\n",
    "r = 1\n",
    "m = ms[case]\n",
    "k = 1\n",
    "\n",
    "radius_train = 0.2\n",
    "radius_test = 0.2\n",
    "\n",
    "batch_size = 1\n",
    "batch_size2 = 1\n",
    "width = 64\n",
    "ker_width = 256\n",
    "depth = 4\n",
    "edge_features = 6\n",
    "node_features = 6\n",
    "\n",
    "epochs = args.epochs\n",
    "learning_rate = args.lr\n",
    "scheduler_step = 50\n",
    "scheduler_gamma = 0.5\n",
    "inspect_interval = args.inspect_interval\n",
    "\n",
    "\n",
    "runtime = np.zeros(2, )\n",
    "t1 = default_timer()\n",
    "\n",
    "if dataset_type == \"darcy\":\n",
    "    s = int(((241 - 1)/r) + 1)\n",
    "    n = s**2\n",
    "    print('resolution', s)\n",
    "    ntrain = 100\n",
    "    ntest = 100\n",
    "    path = 'neurips1_GKN_s'+str(s)+'_ntrain'+str(ntrain)+'_kerwidth'+str(ker_width) + '_m0' + str(m) + \"_id_\" + args.id\n",
    "    path_model = 'model/' + path\n",
    "    path_train_err = 'results/' + path + 'train.txt'\n",
    "    path_test_err = 'results/' + path + 'test.txt'\n",
    "    path_runtime = 'results/' + path + 'time.txt'\n",
    "    path_image = 'results/' + path\n",
    "\n",
    "    reader = MatReader(TRAIN_PATH)\n",
    "    train_a = reader.read_field('coeff')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "    train_a_smooth = reader.read_field('Kcoeff')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "    train_a_gradx = reader.read_field('Kcoeff_x')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "    train_a_grady = reader.read_field('Kcoeff_y')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "    train_u = reader.read_field('sol')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "\n",
    "    reader.load_file(TEST_PATH)\n",
    "    test_a = reader.read_field('coeff')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "    test_a_smooth = reader.read_field('Kcoeff')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "    test_a_gradx = reader.read_field('Kcoeff_x')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "    test_a_grady = reader.read_field('Kcoeff_y')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "    test_u = reader.read_field('sol')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "elif dataset_type.startswith(\"poisson1.0\"):\n",
    "    s = int(((resolution - 1)/r) + 1)\n",
    "    n = s**2\n",
    "    print('resolution', s)\n",
    "\n",
    "    ntrain = 900\n",
    "    ntest = 100\n",
    "    path = 'poisson_s'+str(s)+'_dataset_' + dataset_type + '_ntrain'+str(ntrain)+'_kerwidth'+str(ker_width) + '_m0' + str(m)\n",
    "    path_model = 'model/' + path\n",
    "    path_train_err = 'results/' + path + 'train.txt'\n",
    "    path_test_err = 'results/' + path + 'test.txt'\n",
    "    path_runtime = 'results/' + path + 'time.txt'\n",
    "    path_image = 'results/' + path\n",
    "\n",
    "    f_all = np.load(DATA_PATH + \"RHS_all.npy\")\n",
    "    sol_all = np.load(DATA_PATH + \"SOL_all.npy\")\n",
    "\n",
    "    f_all = np.load(DATA_PATH + \"RHS_all.npy\")\n",
    "    sol_all = np.load(DATA_PATH + \"SOL_all.npy\")\n",
    "    gblur = GaussianBlur(kernel_size=5, sigma=5)\n",
    "\n",
    "    all_a = f_all[:,:,-1]\n",
    "    all_a_smooth = to_np_array(gblur(torch.tensor(all_a.reshape(all_a.shape[0], resolution, resolution))).flatten(start_dim=1))\n",
    "    all_a_reshape = all_a_smooth.reshape(-1, resolution, resolution)\n",
    "    all_a_gradx = np.concatenate([\n",
    "        all_a_reshape[:,1:2] - all_a_reshape[:,0:1],\n",
    "        (all_a_reshape[:,2:] - all_a_reshape[:,:-2]) / 2,\n",
    "        all_a_reshape[:,-1:] - all_a_reshape[:,-2:-1],\n",
    "    ], 1)\n",
    "    all_a_gradx = all_a_gradx.reshape(-1, n)\n",
    "    all_a_grady = np.concatenate([\n",
    "        all_a_reshape[:,:,1:2] - all_a_reshape[:,:,0:1],\n",
    "        (all_a_reshape[:,:,2:] - all_a_reshape[:,:,:-2]) / 2,\n",
    "        all_a_reshape[:,:,-1:] - all_a_reshape[:,:,-2:-1],\n",
    "    ], 2)\n",
    "    all_a_grady = all_a_grady.reshape(-1, n)\n",
    "    all_u = sol_all[:,:,0]\n",
    "\n",
    "    train_a = torch.FloatTensor(all_a[:ntrain])\n",
    "    train_a_smooth = torch.FloatTensor(all_a_smooth[:ntrain])\n",
    "    train_a_gradx = torch.FloatTensor(all_a_gradx[:ntrain])\n",
    "    train_a_grady = torch.FloatTensor(all_a_grady[:ntrain])\n",
    "    train_u = torch.FloatTensor(all_u[:ntrain])\n",
    "\n",
    "    test_a = torch.FloatTensor(all_a[ntrain:])\n",
    "    test_a_smooth = torch.FloatTensor(all_a_smooth[ntrain:])\n",
    "    test_a_gradx = torch.FloatTensor(all_a_gradx[ntrain:])\n",
    "    test_a_grady = torch.FloatTensor(all_a_grady[ntrain:])\n",
    "    test_u = torch.FloatTensor(all_u[ntrain:])\n",
    "\n",
    "else:\n",
    "    raise\n",
    "\n",
    "\n",
    "a_normalizer = GaussianNormalizer(train_a)\n",
    "train_a = a_normalizer.encode(train_a)\n",
    "test_a = a_normalizer.encode(test_a)\n",
    "as_normalizer = GaussianNormalizer(train_a_smooth)\n",
    "train_a_smooth = as_normalizer.encode(train_a_smooth)\n",
    "test_a_smooth = as_normalizer.encode(test_a_smooth)\n",
    "agx_normalizer = GaussianNormalizer(train_a_gradx)\n",
    "train_a_gradx = agx_normalizer.encode(train_a_gradx)\n",
    "test_a_gradx = agx_normalizer.encode(test_a_gradx)\n",
    "agy_normalizer = GaussianNormalizer(train_a_grady)\n",
    "train_a_grady = agy_normalizer.encode(train_a_grady)\n",
    "test_a_grady = agy_normalizer.encode(test_a_grady)\n",
    "\n",
    "u_normalizer = UnitGaussianNormalizer(train_u)\n",
    "train_u = u_normalizer.encode(train_u)\n",
    "# test_u = y_normalizer.encode(test_u)\n",
    "\n",
    "\n",
    "\n",
    "meshgenerator = RandomMeshGenerator([[0,1],[0,1]],[s,s], sample_size=m)\n",
    "data_train = []\n",
    "for j in range(ntrain):\n",
    "    for i in range(k):\n",
    "        idx = meshgenerator.sample()\n",
    "        grid = meshgenerator.get_grid()\n",
    "        edge_index = meshgenerator.ball_connectivity(radius_train)\n",
    "        edge_attr = meshgenerator.attributes(theta=train_a[j,:])\n",
    "        #data_train.append(Data(x=init_point.clone().view(-1,1), y=train_y[j,:], edge_index=edge_index, edge_attr=edge_attr))\n",
    "        data_train.append(Data(x=torch.cat([grid, train_a[j, idx].reshape(-1, 1),\n",
    "                                            train_a_smooth[j, idx].reshape(-1, 1), train_a_gradx[j, idx].reshape(-1, 1),\n",
    "                                            train_a_grady[j, idx].reshape(-1, 1)\n",
    "                                            ], dim=1),\n",
    "                               y=train_u[j, idx], edge_index=edge_index, edge_attr=edge_attr, sample_idx=idx\n",
    "                               ))\n",
    "\n",
    "\n",
    "meshgenerator = RandomMeshGenerator([[0,1],[0,1]],[s,s], sample_size=m)\n",
    "data_test = []\n",
    "for j in range(ntest):\n",
    "    idx = meshgenerator.sample(is_random=False)\n",
    "    grid = meshgenerator.get_grid()\n",
    "    edge_index = meshgenerator.ball_connectivity(radius_test)\n",
    "    edge_attr = meshgenerator.attributes(theta=test_a[j,:])\n",
    "    data_test.append(Data(x=torch.cat([grid, test_a[j, idx].reshape(-1, 1),\n",
    "                                       test_a_smooth[j, idx].reshape(-1, 1), test_a_gradx[j, idx].reshape(-1, 1),\n",
    "                                       test_a_grady[j, idx].reshape(-1, 1)\n",
    "                                       ], dim=1),\n",
    "                          y=test_u[j, idx], edge_index=edge_index, edge_attr=edge_attr, sample_idx=idx\n",
    "                          ))\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size2, shuffle=False)\n",
    "\n",
    "t2 = default_timer()\n",
    "\n",
    "print('preprocessing finished, time used:', t2-t1)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = KernelNN3(width, ker_width,depth,edge_features,in_width=node_features).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "u_normalizer.cuda()\n",
    "ttrain = np.zeros((epochs, ))\n",
    "ttest = np.zeros((epochs,))\n",
    "model.train()\n",
    "\n",
    "# Load model:\n",
    "filename = f\"poisson_s{resolution}_dataset_{dataset_type}_ntrain900_kerwidth256_m0200\"\n",
    "data_record = pickle.load(open(f\"model/{filename}\", \"rb\"))\n",
    "model.load_state_dict(data_record[\"state_dict\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6d7d4-5894-4308-820d-b321f67c4b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isplot = True\n",
    "if isplot:\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(f\"model/analysis_{filename}.pdf\")\n",
    "analysis_record = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for ii, data in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        out = u_normalizer.decode(out.view(batch_size2,-1), sample_idx=data.sample_idx.view(batch_size2,-1))\n",
    "        a_ori = a_normalizer.decode(data.x[:,2].view(1,-1))\n",
    "        l2_item = myloss(out, data.y.view(batch_size2, -1)).item()\n",
    "        mae_item = nn.L1Loss()(out, data.y.view(batch_size2, -1)).item()\n",
    "        record_data(analysis_record, [l2_item, mae_item], [\"L2\", \"MAE\"])\n",
    "        if isplot:\n",
    "            fig = plot(out, data.y, a_ori=a_ori, resolution=resolution, diff_v_limit=0.0075, title=f\"{901+ii}\")\n",
    "            pdf.savefig(fig)\n",
    "if isplot:\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb01b83-19e5-4ca8-a634-b92eae04fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 14\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(analysis_record[\"MAE\"], analysis_record[\"L2\"], s=6)\n",
    "plt.xlabel(\"MAE\", fontsize=fontsize)\n",
    "plt.ylabel(\"L2\", fontsize=fontsize)\n",
    "plt.tick_params(labelsize=fontsize)\n",
    "plt.title(f\"L2 = {np.mean(analysis_record['L2']):.6f}\" + r\" $\\pm$ \" + f\"{np.std(analysis_record['L2']):.6f},   \"\n",
    "          f\"MAE = {np.mean(analysis_record['MAE']):.6f}\" + r\" $\\pm$ \" + f\"{np.std(analysis_record['MAE']):.6f}\")\n",
    "plt.savefig(f\"model/plot_{filename}.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b9089-d6dc-4e0e-8263-d8d6c02ee174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32:\n",
    "fontsize = 14\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(analysis_record[\"MAE\"], analysis_record[\"L2\"], s=6)\n",
    "plt.xlabel(\"MAE\", fontsize=fontsize)\n",
    "plt.ylabel(\"L2\", fontsize=fontsize)\n",
    "plt.tick_params(labelsize=fontsize)\n",
    "plt.title(f\"L2 = {np.mean(analysis_record['L2']):.6f}\" + r\" $\\pm$ \" + f\"{np.std(analysis_record['L2']):.6f},   \"\n",
    "          f\"MAE = {np.mean(analysis_record['MAE']):.6f}\" + r\" $\\pm$ \" + f\"{np.std(analysis_record['MAE']):.6f}\")\n",
    "plt.savefig(f\"model/plot_{filename}.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
