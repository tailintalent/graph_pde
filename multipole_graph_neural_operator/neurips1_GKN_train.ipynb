{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26e18b-ae4f-492e-a0ad-aa6fda1dbc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint as pp\n",
    "import scipy.io\n",
    "from timeit import default_timer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "from utilities import *\n",
    "from nn_conv import NNConv, NNConv_old\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..', '..'))\n",
    "from foundation_pdes.pytorch_net.util import record_data, to_cpu, to_np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8579c-f8f2-44d6-80db-b154127e2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelNN3(torch.nn.Module):\n",
    "    def __init__(self, width_node, width_kernel, depth, ker_in, in_width=1, out_width=1):\n",
    "        super(KernelNN3, self).__init__()\n",
    "        self.depth = depth\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(in_width, width_node)\n",
    "\n",
    "        kernel = DenseNet([ker_in, width_kernel // 2, width_kernel, width_node**2], torch.nn.ReLU)\n",
    "        self.conv1 = NNConv_old(width_node, width_node, kernel, aggr='mean')\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(width_node, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.fc1(x)\n",
    "        for k in range(self.depth):\n",
    "            x = self.conv1(x, edge_index, edge_attr)\n",
    "            if k != self.depth - 1:\n",
    "                x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207197b-9d5a-4832-9bde-5f8e39340c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = \"../../data/Poisson1.0_64/files/\"\n",
    "\n",
    "# f_all = []\n",
    "# for index in range(1, 1001):\n",
    "#     f_pd = pd.read_table(\n",
    "#         DATA_PATH + f'RHS_{index:04d}.txt',\n",
    "#         delimiter=' ',\n",
    "#         skipinitialspace=True,\n",
    "#         header=None,\n",
    "#         names=[\"x\", \"y\", \"f\"],\n",
    "#         index_col=False,\n",
    "#     )                \n",
    "#     f_all.append(f_pd.values)\n",
    "# f_all = np.stack(f_all)\n",
    "\n",
    "# sol_all = []\n",
    "# for index in range(1, 1001):\n",
    "#     sol_pd = pd.read_table(DATA_PATH + f'SOL_{index:04d}.txt',\n",
    "#         delimiter=' ',\n",
    "#         skipinitialspace=True,\n",
    "#         header=None,\n",
    "#         names=[\"sol\"],\n",
    "#         index_col=False,\n",
    "#     )\n",
    "#     sol_all.append(sol_pd.values)\n",
    "# sol_all = np.stack(sol_all)\n",
    "\n",
    "# np.save(DATA_PATH + \"RHS_all.npy\", f_all)\n",
    "# np.save(DATA_PATH + \"SOL_all.npy\", sol_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f9551-6bfb-4980-bee9-1c0e5670c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Training')\n",
    "\n",
    "parser.add_argument('--dataset_type', default=\"poisson1.0-32\", type=str,\n",
    "                    help='dataset type')\n",
    "parser.add_argument('--epochs', default=1000, type=int,\n",
    "                    help='Epochs')\n",
    "parser.add_argument('--lr', default=0.0001, type=float,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--inspect_interval', default=100, type=int,\n",
    "                    help='inspect interval')\n",
    "parser.add_argument('--id', default=\"0\", type=str,\n",
    "                    help='ID')\n",
    "try:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    is_jupyter = True\n",
    "    args = parser.parse_args([])\n",
    "except:\n",
    "    args = parser.parse_args()\n",
    "pp.pprint(args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011ddf6-e3c9-4f2b-aa6f-c26495943a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = args.dataset_type\n",
    "\n",
    "TRAIN_PATH = 'data/piececonst_r241_N1024_smooth1.mat'\n",
    "TEST_PATH = 'data/piececonst_r241_N1024_smooth2.mat'\n",
    "\n",
    "ms = [200]\n",
    "case = 0\n",
    "r = 1\n",
    "m = ms[case]\n",
    "k = 1\n",
    "\n",
    "radius_train = 0.2\n",
    "radius_test = 0.2\n",
    "\n",
    "batch_size = 1\n",
    "batch_size2 = 1\n",
    "width = 64\n",
    "ker_width = 256\n",
    "depth = 4\n",
    "edge_features = 6\n",
    "node_features = 6\n",
    "\n",
    "epochs = args.epochs\n",
    "learning_rate = args.lr\n",
    "scheduler_step = 50\n",
    "scheduler_gamma = 0.5\n",
    "inspect_interval = args.inspect_interval\n",
    "\n",
    "\n",
    "runtime = np.zeros(2, )\n",
    "t1 = default_timer()\n",
    "\n",
    "if dataset_type == \"darcy\":\n",
    "    s = int(((241 - 1)/r) + 1)\n",
    "    n = s**2\n",
    "    print('resolution', s)\n",
    "    ntrain = 100\n",
    "    ntest = 100\n",
    "    path = 'neurips1_GKN_s'+str(s)+'_ntrain'+str(ntrain)+'_kerwidth'+str(ker_width) + '_m0' + str(m) + \"_id_\" + args.id\n",
    "    path_model = 'model/' + path\n",
    "    path_train_err = 'results/' + path + 'train.txt'\n",
    "    path_test_err = 'results/' + path + 'test.txt'\n",
    "    path_runtime = 'results/' + path + 'time.txt'\n",
    "    path_image = 'results/' + path\n",
    "\n",
    "    reader = MatReader(TRAIN_PATH)\n",
    "    train_a = reader.read_field('coeff')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "    train_a_smooth = reader.read_field('Kcoeff')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "    train_a_gradx = reader.read_field('Kcoeff_x')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "    train_a_grady = reader.read_field('Kcoeff_y')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "    train_u = reader.read_field('sol')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "\n",
    "    reader.load_file(TEST_PATH)\n",
    "    test_a = reader.read_field('coeff')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "    test_a_smooth = reader.read_field('Kcoeff')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "    test_a_gradx = reader.read_field('Kcoeff_x')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "    test_a_grady = reader.read_field('Kcoeff_y')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "    test_u = reader.read_field('sol')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "elif dataset_type.startswith(\"poisson1.0\"):\n",
    "    resolution = eval(dataset_type.split(\"-\")[1])\n",
    "    s = int(((resolution - 1)/r) + 1)\n",
    "    n = s**2\n",
    "    print('resolution', s)\n",
    "    DATA_PATH = f\"../../data/Poisson1.0_{resolution}/files/\"\n",
    "\n",
    "    ntrain = 900\n",
    "    ntest = 100\n",
    "    path = 'poisson_s'+str(s)+'_dataset_' + dataset_type + '_ntrain'+str(ntrain)+'_kerwidth'+str(ker_width) + '_m0' + str(m)\n",
    "    path_model = 'model/' + path\n",
    "    path_train_err = 'results/' + path + 'train.txt'\n",
    "    path_test_err = 'results/' + path + 'test.txt'\n",
    "    path_runtime = 'results/' + path + 'time.txt'\n",
    "    path_image = 'results/' + path\n",
    "\n",
    "    f_all = np.load(DATA_PATH + \"RHS_all.npy\")\n",
    "    sol_all = np.load(DATA_PATH + \"SOL_all.npy\")\n",
    "\n",
    "    f_all = np.load(DATA_PATH + \"RHS_all.npy\")\n",
    "    sol_all = np.load(DATA_PATH + \"SOL_all.npy\")\n",
    "    gblur = GaussianBlur(kernel_size=5, sigma=5)\n",
    "\n",
    "    all_a = f_all[:,:,-1]\n",
    "    all_a_smooth = to_np_array(gblur(torch.tensor(all_a.reshape(all_a.shape[0], resolution, resolution))).flatten(start_dim=1))\n",
    "    all_a_reshape = all_a_smooth.reshape(-1, resolution, resolution)\n",
    "    all_a_gradx = np.concatenate([\n",
    "        all_a_reshape[:,1:2] - all_a_reshape[:,0:1],\n",
    "        (all_a_reshape[:,2:] - all_a_reshape[:,:-2]) / 2,\n",
    "        all_a_reshape[:,-1:] - all_a_reshape[:,-2:-1],\n",
    "    ], 1)\n",
    "    all_a_gradx = all_a_gradx.reshape(-1, n)\n",
    "    all_a_grady = np.concatenate([\n",
    "        all_a_reshape[:,:,1:2] - all_a_reshape[:,:,0:1],\n",
    "        (all_a_reshape[:,:,2:] - all_a_reshape[:,:,:-2]) / 2,\n",
    "        all_a_reshape[:,:,-1:] - all_a_reshape[:,:,-2:-1],\n",
    "    ], 2)\n",
    "    all_a_grady = all_a_grady.reshape(-1, n)\n",
    "    all_u = sol_all[:,:,0]\n",
    "\n",
    "    train_a = torch.FloatTensor(all_a[:ntrain])\n",
    "    train_a_smooth = torch.FloatTensor(all_a_smooth[:ntrain])\n",
    "    train_a_gradx = torch.FloatTensor(all_a_gradx[:ntrain])\n",
    "    train_a_grady = torch.FloatTensor(all_a_grady[:ntrain])\n",
    "    train_u = torch.FloatTensor(all_u[:ntrain])\n",
    "\n",
    "    test_a = torch.FloatTensor(all_a[ntrain:])\n",
    "    test_a_smooth = torch.FloatTensor(all_a_smooth[ntrain:])\n",
    "    test_a_gradx = torch.FloatTensor(all_a_gradx[ntrain:])\n",
    "    test_a_grady = torch.FloatTensor(all_a_grady[ntrain:])\n",
    "    test_u = torch.FloatTensor(all_u[ntrain:])\n",
    "\n",
    "else:\n",
    "    raise\n",
    "\n",
    "\n",
    "a_normalizer = GaussianNormalizer(train_a)\n",
    "train_a = a_normalizer.encode(train_a)\n",
    "test_a = a_normalizer.encode(test_a)\n",
    "as_normalizer = GaussianNormalizer(train_a_smooth)\n",
    "train_a_smooth = as_normalizer.encode(train_a_smooth)\n",
    "test_a_smooth = as_normalizer.encode(test_a_smooth)\n",
    "agx_normalizer = GaussianNormalizer(train_a_gradx)\n",
    "train_a_gradx = agx_normalizer.encode(train_a_gradx)\n",
    "test_a_gradx = agx_normalizer.encode(test_a_gradx)\n",
    "agy_normalizer = GaussianNormalizer(train_a_grady)\n",
    "train_a_grady = agy_normalizer.encode(train_a_grady)\n",
    "test_a_grady = agy_normalizer.encode(test_a_grady)\n",
    "\n",
    "u_normalizer = UnitGaussianNormalizer(train_u)\n",
    "train_u = u_normalizer.encode(train_u)\n",
    "\n",
    "\n",
    "\n",
    "meshgenerator = RandomMeshGenerator([[0,1],[0,1]],[s,s], sample_size=m)\n",
    "data_train = []\n",
    "for j in range(ntrain):\n",
    "    for i in range(k):\n",
    "        idx = meshgenerator.sample()\n",
    "        grid = meshgenerator.get_grid()\n",
    "        edge_index = meshgenerator.ball_connectivity(radius_train)\n",
    "        edge_attr = meshgenerator.attributes(theta=train_a[j,:])\n",
    "        #data_train.append(Data(x=init_point.clone().view(-1,1), y=train_y[j,:], edge_index=edge_index, edge_attr=edge_attr))\n",
    "        data_train.append(Data(x=torch.cat([grid, train_a[j, idx].reshape(-1, 1),\n",
    "                                            train_a_smooth[j, idx].reshape(-1, 1), train_a_gradx[j, idx].reshape(-1, 1),\n",
    "                                            train_a_grady[j, idx].reshape(-1, 1)\n",
    "                                            ], dim=1),\n",
    "                               y=train_u[j, idx], edge_index=edge_index, edge_attr=edge_attr, sample_idx=idx\n",
    "                               ))\n",
    "\n",
    "\n",
    "meshgenerator = RandomMeshGenerator([[0,1],[0,1]],[s,s], sample_size=m)\n",
    "data_test = []\n",
    "for j in range(ntest):\n",
    "    idx = meshgenerator.sample()\n",
    "    grid = meshgenerator.get_grid()\n",
    "    edge_index = meshgenerator.ball_connectivity(radius_test)\n",
    "    edge_attr = meshgenerator.attributes(theta=test_a[j,:])\n",
    "    data_test.append(Data(x=torch.cat([grid, test_a[j, idx].reshape(-1, 1),\n",
    "                                       test_a_smooth[j, idx].reshape(-1, 1), test_a_gradx[j, idx].reshape(-1, 1),\n",
    "                                       test_a_grady[j, idx].reshape(-1, 1)\n",
    "                                       ], dim=1),\n",
    "                          y=test_u[j, idx], edge_index=edge_index, edge_attr=edge_attr, sample_idx=idx\n",
    "                          ))\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size2, shuffle=False)\n",
    "\n",
    "t2 = default_timer()\n",
    "\n",
    "print('preprocessing finished, time used:', t2-t1)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = KernelNN3(width, ker_width,depth,edge_features,in_width=node_features).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "u_normalizer.cuda()\n",
    "ttrain = np.zeros((epochs, ))\n",
    "ttest = np.zeros((epochs,))\n",
    "model.train()\n",
    "\n",
    "data_record = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebe4be-af39-4a88-b176-0076ab586b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ep in range(epochs):\n",
    "    t1 = default_timer()\n",
    "    train_mse = 0.0\n",
    "    train_l2 = 0.0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        mse = F.mse_loss(out.view(-1, 1), batch.y.view(-1,1))\n",
    "        mse.backward()\n",
    "\n",
    "        l2 = myloss(\n",
    "            u_normalizer.decode(out.view(batch_size, -1), sample_idx=batch.sample_idx.view(batch_size, -1)),\n",
    "            u_normalizer.decode(batch.y.view(batch_size, -1), sample_idx=batch.sample_idx.view(batch_size, -1)))\n",
    "        optimizer.step()\n",
    "        train_mse += mse.item()\n",
    "        train_l2 += l2.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    t2 = default_timer()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            out = u_normalizer.decode(out.view(batch_size2,-1), sample_idx=batch.sample_idx.view(batch_size2,-1))\n",
    "            test_l2 += myloss(out, batch.y.view(batch_size2, -1)).item()\n",
    "\n",
    "    t3 = default_timer()\n",
    "    ttrain[ep] = train_l2/(ntrain * k)\n",
    "    ttest[ep] = test_l2/ntest\n",
    "\n",
    "    print(f\"Epoch {ep:03d}     train_MSE: {train_mse/len(train_loader):.6f}  \\t train_L2: {train_l2/(ntrain * k):.6f}\\t test_L2: {test_l2/ntest:.6f}\")\n",
    "    record_data(data_record, [ep, train_mse/len(train_loader), train_l2/(ntrain * k), test_l2/ntest], [\"epoch\", \"train_MSE\", \"train_L2\", \"test_L2\"])\n",
    "    if ep % inspect_interval == 0 or ep == epochs - 1:\n",
    "        record_data(data_record, [ep, to_cpu(model.state_dict())], [\"save_epoch\", \"state_dict\"])\n",
    "        pickle.dump(data_record, open(path_model, \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
