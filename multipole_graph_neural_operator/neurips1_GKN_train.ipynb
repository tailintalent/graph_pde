{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26e18b-ae4f-492e-a0ad-aa6fda1dbc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities import *\n",
    "from nn_conv import NNConv, NNConv_old\n",
    "\n",
    "from timeit import default_timer\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8579c-f8f2-44d6-80db-b154127e2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelNN3(torch.nn.Module):\n",
    "    def __init__(self, width_node, width_kernel, depth, ker_in, in_width=1, out_width=1):\n",
    "        super(KernelNN3, self).__init__()\n",
    "        self.depth = depth\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(in_width, width_node)\n",
    "\n",
    "        kernel = DenseNet([ker_in, width_kernel // 2, width_kernel, width_node**2], torch.nn.ReLU)\n",
    "        self.conv1 = NNConv_old(width_node, width_node, kernel, aggr='mean')\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(width_node, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.fc1(x)\n",
    "        for k in range(self.depth):\n",
    "            x = self.conv1(x, edge_index, edge_attr)\n",
    "            if k != self.depth - 1:\n",
    "                x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011ddf6-e3c9-4f2b-aa6f-c26495943a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/piececonst_r241_N1024_smooth1.mat'\n",
    "TEST_PATH = 'data/piececonst_r241_N1024_smooth2.mat'\n",
    "\n",
    "ms = [200]\n",
    "case = 0\n",
    "r = 1\n",
    "s = int(((241 - 1)/r) + 1)\n",
    "n = s**2\n",
    "m = ms[case]\n",
    "k = 1\n",
    "\n",
    "radius_train = 0.2\n",
    "radius_test = 0.2\n",
    "print('resolution', s)\n",
    "\n",
    "\n",
    "ntrain = 100\n",
    "ntest = 100\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "batch_size2 = 1\n",
    "width = 64\n",
    "ker_width = 256\n",
    "depth = 4\n",
    "edge_features = 6\n",
    "node_features = 6\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.0001\n",
    "scheduler_step = 50\n",
    "scheduler_gamma = 0.5\n",
    "\n",
    "\n",
    "path = 'neurips1_GKN_s'+str(s)+'_ntrain'+str(ntrain)+'_kerwidth'+str(ker_width) + '_m0' + str(m)\n",
    "path_model = 'model/' + path\n",
    "path_train_err = 'results/' + path + 'train.txt'\n",
    "path_test_err = 'results/' + path + 'test.txt'\n",
    "path_runtime = 'results/' + path + 'time.txt'\n",
    "path_image = 'results/' + path\n",
    "\n",
    "runtime = np.zeros(2, )\n",
    "t1 = default_timer()\n",
    "\n",
    "\n",
    "reader = MatReader(TRAIN_PATH)\n",
    "train_a = reader.read_field('coeff')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "train_a_smooth = reader.read_field('Kcoeff')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "train_a_gradx = reader.read_field('Kcoeff_x')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "train_a_grady = reader.read_field('Kcoeff_y')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "train_u = reader.read_field('sol')[:ntrain,::r,::r].reshape(ntrain,-1)\n",
    "\n",
    "reader.load_file(TEST_PATH)\n",
    "test_a = reader.read_field('coeff')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "test_a_smooth = reader.read_field('Kcoeff')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "test_a_gradx = reader.read_field('Kcoeff_x')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "test_a_grady = reader.read_field('Kcoeff_y')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "test_u = reader.read_field('sol')[:ntest,::r,::r].reshape(ntest,-1)\n",
    "\n",
    "\n",
    "a_normalizer = GaussianNormalizer(train_a)\n",
    "train_a = a_normalizer.encode(train_a)\n",
    "test_a = a_normalizer.encode(test_a)\n",
    "as_normalizer = GaussianNormalizer(train_a_smooth)\n",
    "train_a_smooth = as_normalizer.encode(train_a_smooth)\n",
    "test_a_smooth = as_normalizer.encode(test_a_smooth)\n",
    "agx_normalizer = GaussianNormalizer(train_a_gradx)\n",
    "train_a_gradx = agx_normalizer.encode(train_a_gradx)\n",
    "test_a_gradx = agx_normalizer.encode(test_a_gradx)\n",
    "agy_normalizer = GaussianNormalizer(train_a_grady)\n",
    "train_a_grady = agy_normalizer.encode(train_a_grady)\n",
    "test_a_grady = agy_normalizer.encode(test_a_grady)\n",
    "\n",
    "u_normalizer = UnitGaussianNormalizer(train_u)\n",
    "train_u = u_normalizer.encode(train_u)\n",
    "# test_u = y_normalizer.encode(test_u)\n",
    "\n",
    "\n",
    "\n",
    "meshgenerator = RandomMeshGenerator([[0,1],[0,1]],[s,s], sample_size=m)\n",
    "data_train = []\n",
    "for j in range(ntrain):\n",
    "    for i in range(k):\n",
    "        idx = meshgenerator.sample()\n",
    "        grid = meshgenerator.get_grid()\n",
    "        edge_index = meshgenerator.ball_connectivity(radius_train)\n",
    "        edge_attr = meshgenerator.attributes(theta=train_a[j,:])\n",
    "        #data_train.append(Data(x=init_point.clone().view(-1,1), y=train_y[j,:], edge_index=edge_index, edge_attr=edge_attr))\n",
    "        data_train.append(Data(x=torch.cat([grid, train_a[j, idx].reshape(-1, 1),\n",
    "                                            train_a_smooth[j, idx].reshape(-1, 1), train_a_gradx[j, idx].reshape(-1, 1),\n",
    "                                            train_a_grady[j, idx].reshape(-1, 1)\n",
    "                                            ], dim=1),\n",
    "                               y=train_u[j, idx], edge_index=edge_index, edge_attr=edge_attr, sample_idx=idx\n",
    "                               ))\n",
    "\n",
    "\n",
    "meshgenerator = RandomMeshGenerator([[0,1],[0,1]],[s,s], sample_size=m)\n",
    "data_test = []\n",
    "for j in range(ntest):\n",
    "    idx = meshgenerator.sample()\n",
    "    grid = meshgenerator.get_grid()\n",
    "    edge_index = meshgenerator.ball_connectivity(radius_test)\n",
    "    edge_attr = meshgenerator.attributes(theta=test_a[j,:])\n",
    "    data_test.append(Data(x=torch.cat([grid, test_a[j, idx].reshape(-1, 1),\n",
    "                                       test_a_smooth[j, idx].reshape(-1, 1), test_a_gradx[j, idx].reshape(-1, 1),\n",
    "                                       test_a_grady[j, idx].reshape(-1, 1)\n",
    "                                       ], dim=1),\n",
    "                          y=test_u[j, idx], edge_index=edge_index, edge_attr=edge_attr, sample_idx=idx\n",
    "                          ))\n",
    "#\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size2, shuffle=False)\n",
    "\n",
    "t2 = default_timer()\n",
    "\n",
    "print('preprocessing finished, time used:', t2-t1)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = KernelNN3(width, ker_width,depth,edge_features,in_width=node_features).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "u_normalizer.cuda()\n",
    "ttrain = np.zeros((epochs, ))\n",
    "ttest = np.zeros((epochs,))\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebe4be-af39-4a88-b176-0076ab586b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(epochs):\n",
    "    t1 = default_timer()\n",
    "    train_mse = 0.0\n",
    "    train_l2 = 0.0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        mse = F.mse_loss(out.view(-1, 1), batch.y.view(-1,1))\n",
    "        mse.backward()\n",
    "\n",
    "        l2 = myloss(\n",
    "            u_normalizer.decode(out.view(batch_size, -1), sample_idx=batch.sample_idx.view(batch_size, -1)),\n",
    "            u_normalizer.decode(batch.y.view(batch_size, -1), sample_idx=batch.sample_idx.view(batch_size, -1)))\n",
    "        optimizer.step()\n",
    "        train_mse += mse.item()\n",
    "        train_l2 += l2.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    t2 = default_timer()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            out = u_normalizer.decode(out.view(batch_size2,-1), sample_idx=batch.sample_idx.view(batch_size2,-1))\n",
    "            test_l2 += myloss(out, batch.y.view(batch_size2, -1)).item()\n",
    "            # test_l2 += myloss(out.view(batch_size2,-1), y_normalizer.encode(batch.y.view(batch_size2, -1))).item()\n",
    "\n",
    "    t3 = default_timer()\n",
    "    ttrain[ep] = train_l2/(ntrain * k)\n",
    "    ttest[ep] = test_l2/ntest\n",
    "\n",
    "    print(k, ntrain, ep, t2-t1, t3-t2, train_mse/len(train_loader), train_l2/(ntrain * k), test_l2/ntest)\n",
    "\n",
    "runtime[0] = t2-t1\n",
    "runtime[1] = t3-t2\n",
    "np.savetxt(path_train_err, ttrain)\n",
    "np.savetxt(path_test_err, ttest)\n",
    "np.savetxt(path_runtime, runtime)\n",
    "torch.save(model, path_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
